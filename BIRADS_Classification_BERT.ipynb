{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBkCdFEiORzEsNRLAf6ymi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ5MsRXk8ZQF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import spacy\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"D:/Downloads/newData.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Use this code if you want to preprocess the dataset\n",
        "# load english language model and create nlp object from it\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "'''\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "df['preprocessed_txt'] = df['Report'].apply(preprocess)\n",
        "'''\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Report'],\n",
        "    df['BIRADS'],\n",
        "    test_size=0.2,\n",
        "    random_state=2022,\n",
        "    stratify=df['BIRADS']\n",
        ")\n",
        "\n",
        "# Hyperparameters\n",
        "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "MAX_LEN = 128\n",
        "NUM_CLASSES = 5  # From 1 to 5\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "# Prepare custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx] - 1  # Convert labels from 1-5 to 0-4 for BERT compatibility\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Convert datasets to torch Datasets\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer, MAX_LEN)\n",
        "\n",
        "# Data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Define the BERT-based model\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Use pooled output\n",
        "        output = self.dropout(pooled_output)\n",
        "        return self.fc(output)\n",
        "\n",
        "# Instantiate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name=BERT_MODEL_NAME, num_classes=NUM_CLASSES)\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "class_weights = torch.tensor([1.0, 0.5, 1.5, 2.0, 3.0], dtype=torch.float).to(device)  # Update based on class distribution\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_dataloader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert labels back from 0-4 to 1-5\n",
        "predictions = [p+1 for p in predictions]\n",
        "true_labels = [t+1 for t in true_labels]\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(true_labels, predictions, target_names=[str(i+1) for i in range(NUM_CLASSES)]))\n",
        "\n",
        "# Bootstrapping to calculate confidence intervals\n",
        "def bootstrap_confidence_interval(metric_func, true_labels, predictions, num_bootstrap=1000, alpha=0.05):\n",
        "    bootstrapped_scores = []\n",
        "    for _ in range(num_bootstrap):\n",
        "        indices = np.random.randint(0, len(true_labels), len(true_labels))\n",
        "        if len(np.unique(np.array(true_labels)[indices])) < 2:\n",
        "            # Skip resamples that don't have at least two classes present\n",
        "            continue\n",
        "        score = metric_func(np.array(true_labels)[indices], np.array(predictions)[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    lower_bound = np.percentile(sorted_scores, 100 * alpha / 2)\n",
        "    upper_bound = np.percentile(sorted_scores, 100 * (1 - alpha / 2))\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "accuracy_ci = bootstrap_confidence_interval(accuracy_score, true_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy:.4f}, 95% CI: [{accuracy_ci[0]:.4f}, {accuracy_ci[1]:.4f}]\")\n",
        "\n",
        "# Calculate macro-average recall\n",
        "macro_recall = recall_score(true_labels, predictions, average='macro')\n",
        "macro_recall_ci = bootstrap_confidence_interval(lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro'), true_labels, predictions)\n",
        "print(f\"Macro Average Recall: {macro_recall:.4f}, 95% CI: [{macro_recall_ci[0]:.4f}, {macro_recall_ci[1]:.4f}]\")\n"
      ]
    }
  ]
}