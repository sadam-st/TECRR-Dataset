{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNURsWT6eDvy+wfVhesN/Xf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTStI3F387g_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import spacy\n",
        "\n",
        "# Load English language model from Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"D:/Downloads/newData.csv\")\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "df['preprocessed_txt'] = df['Report'].apply(preprocess)\n",
        "\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['preprocessed_txt'],\n",
        "    df['BIRADS'],\n",
        "    test_size=0.2,\n",
        "    random_state=2022,\n",
        "    stratify=df['BIRADS']\n",
        ")\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "NUM_CLASSES = 5\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Prepare custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenization and padding\n",
        "        tokens = self.tokenizer(text)\n",
        "        tokens = tokens[:self.max_len] + [0] * (self.max_len - len(tokens))  # Padding\n",
        "        return {\n",
        "            'text': torch.tensor(tokens, dtype=torch.long),\n",
        "            'label': torch.tensor(label-1, dtype=torch.long)  # Adjust labels from 1-5 to 0-4\n",
        "        }\n",
        "\n",
        "# Tokenizer: a basic word tokenizer\n",
        "def tokenizer(text):\n",
        "    return [vocab.get(word, vocab['<unk>']) for word in text.split()]\n",
        "\n",
        "# Build Vocabulary\n",
        "vocab = {}\n",
        "for text in df['preprocessed_txt']:\n",
        "    for word in text.split():\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) + 1\n",
        "vocab['<unk>'] = 0  # Unknown token\n",
        "\n",
        "# Convert datasets to torch Datasets\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer, MAX_LEN)\n",
        "\n",
        "# Data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = lstm_out[:, -1, :]  # Take output from the last LSTM unit\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMClassifier(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Compute class weights for imbalanced data\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([1, 2, 3, 4, 5]), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        inputs, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_dataloader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Adjust predicted and true labels back to the original range (1 to 5)\n",
        "predictions = [p+1 for p in predictions]\n",
        "true_labels = [t+1 for t in true_labels]\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(true_labels, predictions, target_names=[str(i) for i in range(1, NUM_CLASSES+1)], output_dict=True)\n",
        "print(report)\n",
        "\n",
        "# Bootstrapping for 95% confidence intervals\n",
        "def bootstrap_metric(y_true, y_pred, metric_func, n_bootstrap=1000):\n",
        "    bootstrapped_scores = []\n",
        "    rng = np.random.default_rng()\n",
        "    for _ in range(n_bootstrap):\n",
        "        indices = rng.choice(np.arange(len(y_true)), len(y_true), replace=True)\n",
        "        score = metric_func(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "    sorted_scores = np.sort(bootstrapped_scores)\n",
        "    ci_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
        "    ci_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
        "    return np.mean(bootstrapped_scores), ci_lower, ci_upper\n",
        "\n",
        "# Accuracy confidence interval\n",
        "accuracy_mean, accuracy_lower, accuracy_upper = bootstrap_metric(np.array(true_labels), np.array(predictions), accuracy_score)\n",
        "print(f\"Accuracy: {accuracy_mean:.4f} (95% CI: {accuracy_lower:.4f} - {accuracy_upper:.4f})\")\n",
        "\n",
        "# Macro recall confidence interval\n",
        "macro_recall_mean, macro_recall_lower, macro_recall_upper = bootstrap_metric(np.array(true_labels), np.array(predictions),\n",
        "                                                                            lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro'))\n",
        "print(f\"Macro Recall: {macro_recall_mean:.4f} (95% CI: {macro_recall_lower:.4f} - {macro_recall_upper:.4f})\")\n"
      ]
    }
  ]
}